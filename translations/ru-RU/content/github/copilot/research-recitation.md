---
title: Анализ повторений
intro: A first look at rote learning in {% data variables.product.prodname_dotcom %} Copilot suggestions.
redirect_from:
- /early-access/github/copilot/research-recitation
versions:
  fpt: '*'
ms.openlocfilehash: cacf9a63013c5bbf9b7d867e088640ff01400289
ms.sourcegitcommit: 67064b14c9d4d18819db8f6398358b77a1c8002a
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/17/2022
ms.locfileid: "145069765"
---
Автор: Альберт Циглер (Albert Ziegler) (@wunderalbert)

## <a name="-data-variablesproductprodname_dotcom--copilot-parrot-or-crow"></a>{% data variables.product.prodname_dotcom %} Copilot: попугай или ворон?
Первый взгляд на обучение {% data variables.product.prodname_dotcom %} Copilot.

## <a name="introduction"></a>Введение

{% data variables.product.prodname_dotcom %} Copilot обучается на миллиардах строк открытого кода. Его предложения адаптируются к вашему коду, но подбираются с учетом кода, написанного другими пользователями.

Насколько прямая связь между предлагаемым кодом и кодом, на базе которого строится предложение? В недавней новаторской работе <sup id="anchor1">[1](#footnote1)</sup>, Bender, Gebru et al. было предложено определение "стохастические попугаи" для систем искусственного интеллекта, аналогичных тем, что лежат в основе {% data variables.product.prodname_dotcom %} Copilot. Или, как заметил один специалист по машинному обучению в {% data variables.product.company_short %}<sup id="anchor2">[2](#footnote2)</sup> во время неформальной беседы: эти системы ведут себя как "младенец с фотографической памятью".

Это чрезмерные упрощения, и они сделаны сознательно. Многие предложения {% data variables.product.prodname_dotcom %} Copilot очень хорошо адаптируются к конкретной базе кода, над которым работает пользователь. Часто они похожи не попугайские повторения, а скорее на работу ворона, собирающего новые инструменты из небольших блоков<sup id="anchor3">[3](#footnote3)</sup>. Не нельзя отрицать, что у {% data variables.product.prodname_dotcom %} Copilot огромная память:

![Видеодемонстрация Copilot](/assets/images/help/copilot/resources_recitation_example_zen.gif)

Здесь я намеренно поручил <sup id="anchor4">[4](#footnote4)</sup> {% data variables.product.prodname_dotcom %} Copilot продекламировать хорошо известный текст, который он явно знает наизусть. Я тоже помню пару текстов наизусть. Например, я все еще помню некоторые стихи, которые выучил в школе. Тем не менее независимо от темы разговора мне ни разу не хотелось учудить, начав вещать четырехстопным ямбом о нарциссах.

Можно ли сказать, что в отличие от меня {% data variables.product.prodname_dotcom %} Copilot склонен к такому поведению? Сколько из его предложений уникальны, и как часто он просто бездумно повторяет похожий код, который ему попадался во время обучения?

## <a name="the-experiment"></a>Эксперимент

Во время ранней разработки {% data variables.product.prodname_dotcom %} Copilot почти 300 сотрудников использовали его в повседневной работе в рамках внутренней пробной версии. В ходе этой работы был сформирован хороший набор данных для тестирования на повторяемость. Я хотел узнать, как часто {% data variables.product.prodname_dotcom %} Copilot давал программистам предложения, которые копировались из ранее просмотренного кода.

Я ограничил исследование предложениями на Python, сделанными до 7 мая 2021 года (в тот день мы начали извлекать эти данные). В результате у нас оказалось 453 780 предложений, сделанных за 396 "пользовательских недель", т. е. календарных недель, в течение которых разработчики активно использовали {% data variables.product.prodname_dotcom %} Copilot в коде Python.

### <a name="automatic-filtering"></a>Автоматическая фильтрация

453 780 предложений — это много, но многие из них можно сразу же отбросить. Чтобы перейти к интересным случаям, рассмотрим последовательности "слов", которые идут в предложении в том же порядке, что и в коде, на котором обучался {% data variables.product.prodname_dotcom %} Copilot. В этом контексте знаки препинания, квадратные скобки и другие специальные символы считаются "словами", а табуляции, пробелы или даже разрывы строк полностью игнорируются. В конце концов, цитата не меняется, если перед ней идет 1 табуляция или 8 пробелов.

Например, одним из предложений Copilot {% data variables.product.prodname_dotcom %} было следующее регулярное выражение для чисел, разделенных пробелами:

```
r'^\s+\d+\s+\d+\s+\d+\s+\d+\s+\d+\s+\d+\s+\d+\s+\d+\s+\d+\s+\d+\s+\d+\s+\d+\s+\d+\s+\d+\s+\d+\s+\d+'
```

В приведенном выше смысле это ровно 100 "слов", однако это очень концентрированный пример: средняя непустая строка кода содержит только 10 "слов". Я ограничил это исследование случаями, когда перекрытие с кодом, на котором обучался {% data variables.product.prodname_dotcom %} Copilot, составляло минимум 60 таких "слов". Нам нужно было поставить какое-то ограничение. и я думаю, что более короткие последовательности довольно редко могут быть интересными. На самом деле, большинство интересных случаев, которые мы получили позже, были намного длиннее порогового значения в 60 слов.

Если перекрытие включает код, который пользователь уже написал, это также учитывалось в подсчете длины. В конце концов, пользователь мог написать этот контекст также с помощью {% data variables.product.prodname_dotcom %} Copilot.

В следующем примере пользователь начал писать очень распространенный фрагмент кода. {% data variables.product.prodname_dotcom %} Copilot его завершает. Несмотря на то, что сам завершающий код довольно короткий, вместе с уже существующим кодом его длина значительно превышает пороговое значение, поэтому он был включен в рассматриваемые случаи. 

![Пример кода](/assets/images/help/copilot/example_last_straw.png)

Эта процедура достаточно мягкая и позволяет включать в выборку множество относительно "скучных" примеров, как оба рассмотренных выше. Однако она все же вполне эффективна при подготовке интересных случаев для анализа человеком и позволяет отфильтровывать более 99 % предложений Copilot.

### <a name="manual-bucketing"></a>Группировка вручную

После фильтрации осталось 473 предложения. Но они имели очень разные формы:

1. Некоторые представляли собой повторы одного предложения, которое прошло фильтрацию. Например, иногда {% data variables.product.prodname_dotcom %} Copilot выдает предложение, разработчик вводит строку комментария, а {% data variables.product.prodname_dotcom %} Copilot выдает очень похожее предложение снова. Я удалил эти случаи из анализа как дубликаты.
2. Некоторые из этих последовательностей были длинными и повторяющимися. Как в следующем примере, где повторяющиеся блоки `‘<p>’`, наверняка, попадались в обучающем наборе: <br>![Примеры повторений](/assets/images/help/copilot/example_repetitions.png)<br> Такие предложения могут быть полезны (тестовые случаи, регулярные выражения) или нет (как, например, этот вариант). В любом случае, они не были прямыми повторами, которые я имел в виду в начале исследования.
3. Некоторые из них были стандартными ресурсами, такими как натуральные или простые числа, биржевые тикеры или греческий алфавит: <br>![Пример греческого алфавита](/assets/images/help/copilot/example_greek.png)
4. Некоторые из них представляли собой обычные, простые и, возможно, даже универсальные способы решения тех или иных задач, где в силу естественных причин не может быть больших различий. Например, в средней части следующего фрагмента можно увидеть очень стандартный способ использования пакета BeautifulSoup для анализа списка в Википедии. Кстати, в самом похожем фрагменте, найденном в тренировочных данных {% data variables.product.prodname_dotcom %} Copilot<sup id="anchor5">[5](#footnote5)</sup>, такой код используется для анализа другой статьи и выполняет другие действия с результатами. <br>![Пример BeautifulSoup](/assets/images/help/copilot/example_beautiful_soup.png) <br>Это также не соответствует моему представлению о прямом цитировании. Это больше похоже на то, когда кто-то говорит: "Я — выбрасывать мусор. Скоро буду" — это фактическое заявление, а не цитата, хотя эту же конкретную фразу уже произносили много раз до этого.
5. Затем идут все остальные случаи. В них есть хотя бы какое-то перекрытие в коде или примечаниях. Именно они интересуют меня больше всего, и на них я сосредоточусь в дальнейшем.

При такой группировке неизбежно имеются некоторые пограничные случаи<sup id="anchor6">[6](#footnote6)</sup>, и они зависят от вашего представления о классификации. Может быть, вы даже не согласны с такой группировкой в целом.

Поэтому мы сделали этот набор данных общедоступным<sup id="anchor7">[7](#footnote7)</sup>. Так что если вы считаете, что группировать нужно было по-другому или если вас интересуют другие аспекты копирования примеров из обучающего набора GitHub Copilot, просто игнорируйте следующий раздел и делайте собственные выводы.

## <a name="results"></a>Результаты

![Обзорная диаграмма](/assets/images/help/copilot/plot_buckets.png)

Для большинства предложений {% data variables.product.prodname_dotcom %} Copilot наш автоматический фильтр не обнаружил существенного перекрытия c тренировочным кодом. Однако мы все же получили 473 интересных случая. После удаления первой группы (варианты, которые очень похожи на другие случаи), у меня осталось 185 предложений. Из них 144 были отсортированы в группах 2–4. В результате в последней группе остался 41 пример — "повторы", в том значении термина, которое я имею в виду.

Это соответствует **1 повторению каждые 10 пользовательских недель** (95%-ный доверительный интервал: 7–13 недель по пуассоновскому критерию).

Конечно, это проверялось на разработчиках {% data variables.product.prodname_dotcom %} и Майкрософт, которые испытывали {% data variables.product.prodname_dotcom %} Copilot в деле. Если ваш стиль программирования сильно отличается от их стиля, ваши результаты могут быть другими. В частности, некоторые из этих разработчиков используют Python только периодически. Мне не удалось определить, насколько часто, поэтому я считал пользователями всех, кто в течение недели пишет на Python хоть что-то.

1 событие за 10 недель — кажется немного, но это и не 0. Меня удивили три вещи.

### <a name="-data-variablesproductprodname_dotcom--copilot-quotes-when-it-lacks-specific-context"></a>{% data variables.product.prodname_dotcom %} Copilot выдает точные повторы при отсутствии конкретного контекста

Если я хочу запомнить текст песни, мну нужно прослушать ее много раз. {% data variables.product.prodname_dotcom %} Copilot ничем не отличается от меня: чтобы запомнить фрагмент кода наизусть, он должен часто встречать этот фрагмент кода. Каждый файл показывается для {% data variables.product.prodname_dotcom %} Copilot только один раз, поэтому фрагмент должен присутствовать во многих разных файлах в общедоступном коде.

Из 41 основного случая, которые мы выделили во время добавления группирования вручную, все присутствуют минимум в 10 разных файлах. Большинство из них (35 случаев) появляются более ста раз. Однажды {% data variables.product.prodname_dotcom %} Copilot предложил начать пустой файл текстом, который он видел более 700 000 раз во время обучения — это была универсальная общедоступная лицензия GNU.

На следующей диаграмме показано количество подходящих файлов с результатами в группе 5 (одна красная метка внизу для каждого результата) по сравнению с группами 2–4. Я не учитывал группу 1, которая, по сути, представляет собой набор дубликатов для примеров из групп 2–4 и 5. Итоговое распределение отображается красной линией, которая достигает пика в диапазоне от 100 до 1000 совпадений.

![Диаграмма по количеству совпадений](/assets/images/help/copilot/plot_copies.png)

### <a name="-data-variablesproductprodname_dotcom--copilot-mostly-quotes-in-generic-contexts"></a>{% data variables.product.prodname_dotcom %} Copilot в основном выдает точные цитаты в универсальных контекстах

Со временем каждый файл становится уникальным. Однако {% data variables.product.prodname_dotcom %} Copilot не ждет этого <sup id="anchor8">[8](#footnote8)</sup>: он выдает предложения, пока файл еще очень универсальный. А в отсутствие конкретики, гораздо вероятнее, что инструмент будет выдавать точные повторения из других источников.

![Диаграмма по длине контекста](/assets/images/help/copilot/plot_context.png)

Конечно, разработчики программного обеспечения проводят большую часть времени, работая с файлами с достаточно уникальным контекстом, чтобы {% data variables.product.prodname_dotcom %} Copilot мог давать уникальные предложения. Напротив, предложения в начале носят довольно случайный характер, так как {% data variables.product.prodname_dotcom %} Copilot не знает, какой будет программа. Но иногда, особенно в небольших проектах или автономных сценариях, небольшого объема контекста может быть достаточно, чтобы догадаться, что хочет сделать пользователь. А иногда контекст все равно остается слишком универсальным, и {% data variables.product.prodname_dotcom %} Copilot думает, что одно из точных повторений может оказаться к месту:

![Пример кода](/assets/images/help/copilot/example_robot.png)

Это практически точное повторение фрагмента из курсовой работы по классу робототехники, загруженной в различных вариациях<sup id="anchor9">[9](#footnote9)</sup>.

### <a name="detection-is-only-as-good-as-the-tool-that-does-the-detecting"></a>Качество обнаружения определяется качеством инструмента, который его выполняет

В текущей форме фильтр выдает большое количество неинтересных случаев. Однако шума должно быть не слишком много. Для участников эксперимента в среднем было немногим более одного подходящего предложения в неделю (хотя, вероятно, в виде всплесков). Из них около 17 % (95%-ный доверительный интервал с биномиальным критерием: 14–21%) будет находиться в пятой группе.

И, разумеется, никогда нельзя исключать возможность ошибки: и здесь она тоже может произойти. Некоторые случаи довольно трудно обнаружить с помощью средства, которое мы создаем, однако они имеют очевидный источник. Возвращаясь к мудрости дзен на Python:

![Вариант дзен](/assets/images/help/copilot/resources_recitation_example_zen_caw.gif)

## <a name="conclusion-and-next-steps"></a>Заключение и дальнейшие действия

В этом исследовании показано, что {% data variables.product.prodname_dotcom %} Copilot _может_ предлагать точные повторения кода, однако делает он это редко, а когда делает, то в основном цитирует код, который цитируют все, и в основном допускает это в начале файла.

При этом между GitHub Copilot, цитирующим код, и мной, цитирующим стихотворение, есть большая разница: я _понимаю_, что я цитирую. Мне бы также хотелось знать, когда Copilot повторяет существующий код, а не придумывает собственный. В этом случае я смогу найти сведения об этом коде, а также упомянуть оригинальных разработчиков, если это необходимо.

Это решается просто — совместное использование предварительной фильтрации, которую мы применяли в этом анализе для обнаружения перекрытий с обучающим набором. Если предложение содержит фрагменты кода, скопированные из обучающего набора, в пользовательском интерфейсе нужно просто указать, откуда он взят. После этого можно либо указать авторов, либо полностью отказаться от использования этого кода.

Такой поиск дубликатов еще не интегрирован в техническую версию, но мы планируем это сделать. И мы будем продолжать работать над снижением объема повторов, а также над тем, чтобы уточнить их обнаружение.

<br><br>

### <a name="footnotes"></a>Примечания

<a name="footnote1">1</a>: [Об опасности стохастических попугаев: могут ли языковые модели быть слишком большими?](https://dl.acm.org/doi/10.1145/3442188.3445922) [^](#anchor1)

<a name="footnote2">2</a>: [Тиферет Газит (Tiferet Gazit)](https://github.com/tiferet)[^](#anchor2)

<a name="footnote3">3</a>: см. von Bayern et al. о творческой мудрости ворон: [создание сложных инструментов новокаледонскими воронами](https://www.nature.com/articles/s41598-018-33458-z)[^](#anchor3)

<a name="footnote4">4</a>: см. Carlini et al. о намеренном запуске отзыва обучающих данных: [Извлечение обучающих данных из больших языковых моделей](https://arxiv.org/pdf/2012.07805.pdf)[^](#anchor4)

<a name="footnote5">5</a>: jaeteekae: [DelayedTwitter](https://github.com/jaeteekae/DelayedTwitter/blob/0a0b03de74c03cfbf36877ffded0cb1312d59642/get_top_twitter_accounts.py#L21) [^](#anchor5)

<a name="footnote6">6</a>: Наверное, не _слишком_ много. Я попросил некоторых разработчиков помочь мне классифицировать случаи. Всем им было предложено отмечать, когда они не были уверены в своем решении. Это произошло только в 34 случаях, т. е. менее 10 %. [^](#anchor6)

<a name="footnote7">7</a>. В [общедоступном наборе данных](/assets/images/help/copilot/matched_snippets.csv) я указываю часть предложения Copilot, которая была найдена в обучающем наборе, частоту вхождений и ссылку на пример, где она встречается в общедоступном коде. По соображениям конфиденциальности я не включаю часть предложенного кода, которой нет в общедоступных источниках, и контекст кода, введенный пользователем (указывается только его длина). [^](#anchor7)

<a name="footnote8">8</a>. На самом деле, после завершения этого эксперимента, в {% data variables.product.prodname_dotcom %} Copilot _появилось требование_ к минимальному объему файла. Поэтому некоторые отмеченные здесь предложения, не были бы показаны в текущей версии инструмента. [^](#anchor8)

<a name="footnote9">9</a>: Например, jenevans33: [CS8803-1](https://github.com/jenevans33/CS8803-1/blob/eca1bbc27ca6f7355dbc806b2f95964b59381605/src/Final/ekfcode.py#L23) [^](#anchor9)
